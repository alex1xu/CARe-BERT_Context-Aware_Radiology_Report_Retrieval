{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6628222,"sourceType":"datasetVersion","datasetId":3826278},{"sourceId":6753991,"sourceType":"datasetVersion","datasetId":3885301},{"sourceId":6757867,"sourceType":"datasetVersion","datasetId":3885304},{"sourceId":6758649,"sourceType":"datasetVersion","datasetId":3890414}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## CARe-BERT Pipeline and PubMedBERT Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"Alexander Xu, 2024","metadata":{}},{"cell_type":"markdown","source":"**Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport gzip\nimport tarfile\nfrom joblib import Parallel, delayed\nimport sys\n\nimport random\nimport json\nimport requests\nfrom datetime import datetime\nfrom collections import deque,defaultdict,Counter\n\nfrom IPython.display import clear_output\nfrom IPython.display import FileLink\nfrom tqdm import tqdm\nimport logging\nimport argparse\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport re\n\nimport nltk\nnltk.download('wordnet','/root/nltk_data')\n!unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/\nfrom nltk.stem import WordNetLemmatizer\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import IterableDataset\nfrom torch.utils.data import Dataset\n\n!pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\nfrom sentence_transformers.evaluation import TripletEvaluator\n\nfrom scipy.spatial.distance import cityblock, euclidean, cosine\nfrom scipy.stats import binom\n\ntqdm.pandas()\nrandom.seed(117)\nclear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"markdown","source":"**Utility Functions**","metadata":{}},{"cell_type":"code","source":"# Flatten a matrix\ndef flatten_extend(matrix):\n    flat_list = []\n    for row in matrix:\n        flat_list.extend(row)\n    return flat_list\n\n# Delete folder contents\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MIMIC Dataset Pre-Processing Functions**","metadata":{}},{"cell_type":"code","source":"# Parse MIMIC dataset row\ndef parse_row(data,ent_counts,rel_counts):\n    entities = [\n        {\n            'id': idx,\n            'start': ent[\"start_ix\"],\n            'end': ent[\"end_ix\"] + 1,\n            'label': ent[\"label\"],\n            'text': ent['tokens']\n        }\n        for idx, ent in data.items()\n    ]\n\n    relations = []\n    for idx, ent in data.items():\n        for relation in ent['relations']:\n            head, child, label = relation[1], idx, relation[0]\n            rel_label = 'described_as' if label == 'modify' else label\n            rel_counts[rel_label] += 1\n            relations.append({'head': child if label == 'modify' else head,\n                              'child': head if label == 'modify' else child,\n                              'label': rel_label})\n    \n    return (entities,relations,)\n\n# Parse MIMIC dataset structure\ndef parse_dataset(dataset):\n    ent_counts = defaultdict(int)\n    rel_counts = defaultdict(int)\n    \n    dataset_copy = dataset.copy()\n    dataset_copy['ents'], dataset_copy['rels'] = zip(\n        *tqdm(dataset['data'].apply(lambda data: parse_row(data, ent_counts, rel_counts)))\n    )\n    \n    return dataset_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Knowledge Graph Construction Functions**","metadata":{}},{"cell_type":"code","source":"# Map each node to its original sentence\ndef match_nodes_to_sentences(nodes, sentences):\n    node_to_sentence = {}\n    sentence_boundaries = [0]\n    \n    for sentence in sentences:\n        sentence_boundaries.append(sentence_boundaries[-1] + len(sentence.split()))\n\n    for node in nodes:\n        start = node['start']\n        \n        for idx, boundary in enumerate(sentence_boundaries):\n            if start < boundary:\n                node_to_sentence[node['id']] = idx\n                break\n    \n    return node_to_sentence\n\n# Identify roots of a knowledge graph\ndef find_roots(entities, relations):\n    nodes_with_incoming_edges = {relation['child'] for relation in relations}\n    all_nodes = {entity['id'] for entity in entities}\n    \n    roots = all_nodes - nodes_with_incoming_edges\n    \n    return list(roots)\n\n# Create knowledge graph from root in DFS manner iteratively\ndef fill_from_root(start_node, entities, relations, node_to_sentence):\n    graph = {'nodes': [], 'edges': [], 'sentences': set(), 'root': start_node}\n    visited = set()\n    queue = deque([(start_node, None)])  # (node_id, parent_node_id)\n\n    while queue:\n        node_id, parent_id = queue.popleft()\n        if node_id in visited:\n            continue\n        visited.add(node_id)\n        \n        # Get the entity and add it to the graph\n        entity = next(entity for entity in entities if entity['id'] == node_id)\n        graph['nodes'].append(entity)\n        graph['sentences'].add(node_to_sentence[node_id])\n        \n        # Add the edge from parent to current node\n        if parent_id:\n            label = next(rel['label'] for rel in relations if rel['head'] == parent_id and rel['child'] == node_id)\n            graph['edges'].append({\n                'from_node_id': node_id,\n                'to_node_id': parent_id,\n                'label': label\n            })\n        \n        # Add children to the queue\n        queue.extend((rel['child'], node_id) for rel in relations if rel['head'] == node_id)\n    \n    return graph\n\n# Create all knowledge graphs\ndef create_knowledge_graphs(text, entities, relations):\n    sentences = tokenizer.tokenize(text)\n    node_to_sentence = match_nodes_to_sentences(entities, sentences)\n    \n    roots = find_roots(entities, relations)\n    graphs = [fill_from_root(root, entities, relations, node_to_sentence) for root in roots]\n\n    return graphs\n\n# Main function to preprocess all data\ndef load_MIMIC(filepath, file_name, save_csv=False):\n    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    \n    with open(filepath) as file:\n        json_data = json.load(file)\n        data_mimic = pd.DataFrame(json_data).T\n        data_mimic = data_mimic.rename({'entities': 'data'}, axis=1)\n    \n    data_mimic = parse_dataset(data_mimic)\n    data_mimic['graphs'] = data_mimic.progress_apply(lambda row: create_knowledge_graphs(row['text'], row['ents'], row['rels']), axis=1)\n\n    if save_csv:\n        data_mimic.to_csv(f'/kaggle/working/data/{file_name}.csv', index=False)\n        \n    return data_mimic","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Knowledge Graph Permutation for Document Augmentation (Hard Negatives) and Synthetic Query Generation (Anchors) Functions**","metadata":{}},{"cell_type":"code","source":"# Synthetic query augmentation for a node\ndef visit_node(node,query,vocabulary,replaceables,visited,edge=None):\n    if node['label'] == 'OBS-U':\n        query.append(\"possible\")\n    if node['label'] == 'OBS-DA':\n        query.append(\"no\")\n    query.append(node['text'].lower())\n    \n    if edge:\n        vocabulary[node['label']][edge['label']].add(node['text'].lower())\n    else: \n        vocabulary[node['label']]['described_as'].add(node['text'].lower())\n        vocabulary[node['label']]['suggestive_of'].add(node['text'].lower())\n        vocabulary[node['label']]['located_at'].add(node['text'].lower())\n    \n    replaceables.append((node['text'], node['label'], edge['label'] if edge else None))\n    visited.add(node['id'])\n\n# Helper function to add children nodes and their relations to the query\ndef add_children_to_query(node, mask, node_id_to_number, graph, query, vocabulary, replaceables, visited):\n    child_other = False\n    for child_edge in graph['edges']:\n        if (child_edge['from_node_id'] == node['id'] and\n                child_edge['to_node_id'] not in visited and\n                (mask & (1 << node_id_to_number[child_edge['to_node_id']]))):\n            \n            child_node = next(node for node in graph['nodes'] if node['id'] == child_edge['to_node_id'])\n\n            if child_other:\n                query.append('and')\n            child_other = True\n\n            edge_label = child_edge['label']\n            query.append(edge_label.replace('_', ' '))\n\n            visit_node(child_node, query, vocabulary, replaceables, visited, edge=child_edge)\n            add_children_to_query(child_node, mask, node_id_to_number, graph, query, vocabulary, replaceables, visited)\n    \n# Build queries for a knowledge graph\ndef create_queries(graph, document, vocabulary):\n    queries = {}\n    num_nodes = len(graph['nodes'])\n    node_id_to_number = {node['id']: i for i, node in enumerate(graph['nodes'])}\n    \n    for root_node in graph['nodes']:\n        for mask in range(1, 1 << num_nodes):\n            if not (mask & (1 << node_id_to_number[root_node['id']])):\n                continue\n            visited = set()\n            replaceables = []\n            query = []\n            \n            visit_node(root_node, query, vocabulary, replaceables, visited)\n            add_children_to_query(root_node, mask, node_id_to_number, graph, query, vocabulary, replaceables, visited)\n\n            if bin(mask).count('1') == len(visited):\n                lemmatized_query = ' '.join([lemmatizer.lemmatize(token) for token in query]).lower()\n                queries[lemmatized_query] = (document, replaceables)\n    \n    return queries\n            \n# Augment original document to create hard negatives\ndef create_negatives(document, replaceables):\n    changed = False\n    iterations = 0\n    replaceable_dict = {token: (label, edge_label) for token, label, edge_label in replaceables}\n    \n    while not changed and iterations <= MAX_ITERATIONS:\n        iterations += 1\n        res = document.lower().split(' ')\n        probs = binom.rvs(1, REPLACEMENT_PROB, size=len(res))\n        probs_inside = binom.rvs(1, REPLACEMENT_PROB, size=len(res))\n        \n        for idx, cur_token in enumerate(res):\n            if probs[idx] and cur_token in replaceable_dict:\n                label, edge_label = replaceable_dict[cur_token]\n                changed = True\n\n                if label == 'OBS-DP' and probs_inside[idx]:\n                    res[idx] = f\"{random.choice(['no', 'absence of', 'possible'])} {cur_token}\"\n                else:\n                    new_vocab = random.choice(list(vocabulary[label][edge_label])).lower() if edge_label else random.choice(vocabulary[label]['ALL']).lower()\n                    while (new_vocab == cur_token or\n                           stemmer.stem(new_vocab) == stemmer.stem(cur_token) or\n                           lemmatizer.lemmatize(new_vocab) == lemmatizer.lemmatize(cur_token)):\n                        new_vocab = random.choice(list(vocabulary[label][edge_label])).lower() if edge_label else random.choice(vocabulary[label]['ALL']).lower()\n                    res[idx] = new_vocab\n\n    return ' '.join(res)\n\n# Combine negatives, queries, positives (original documents) into triplets\ndef create_triplets(query, embed_docs):\n    query = query.lower()\n    qid = len(query_set)\n    \n    if query not in query_set:\n        queries[qid] = query\n        query_set[query] = qid\n    else:\n        qid = query_set[query]\n        \n    random.shuffle(embed_docs)\n    \n    for document, replaceables in embed_docs[:10]:\n        document = re.sub(r'_+', '_', document).lower()\n        did = len(document_set)\n        \n        if document not in document_set:\n            corpus[did] = document\n            document_set[document] = did\n        else:\n            did = document_set[document]\n        \n        pos[qid].append(did)\n        \n        for _ in range(NEGATIVE_RATIO):\n            neg_did = len(document_set)\n            neg_doc = create_negatives(document, replaceables)\n            \n            if neg_doc not in document_set:\n                corpus[neg_did] = neg_doc\n                document_set[neg_doc] = neg_did\n            else:\n                neg_did = document_set[neg_doc]\n                \n            neg[qid].append(neg_did)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CARe-BERT Workflow","metadata":{}},{"cell_type":"code","source":"# Initialize dictionaries and sets\nvocabulary = {\n    'OBS-DP': {'described_as': set(), 'located_at': set(), 'suggestive_of': set()},\n    'OBS-U': {'described_as': set(), 'located_at': set(), 'suggestive_of': set()},\n    'OBS-DA': {'described_as': set(), 'located_at': set(), 'suggestive_of': set()},\n    'ANAT-DP': {'described_as': set(), 'located_at': set(), 'suggestive_of': set()}\n}\nqueries, corpus, pos, neg = {}, {}, defaultdict(list), defaultdict(list)\nquery_set, document_set, train_queries = {}, {}, {}\nqdr_dict = defaultdict(list)\n\n# Initialize tools\ntokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\nlemmatizer = WordNetLemmatizer()\nstemmer = nltk.stem.PorterStemmer()\n\n# CARe-BERT Pipeline Parameters\nNEGATIVE_RATIO = 5\nMAX_ITERATIONS = 5\nREPLACEMENT_PROB = 0.3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Download MIMIC Dataset and Process Data to Knowledge Forests**","metadata":{}},{"cell_type":"code","source":"# Load datasets\ndata_pipeline = load_MIMIC(\"/kaggle/input/radgraph-data/MIMIC-CXR_graphs.json\", \"MIMIC_inference\", save_csv=True)\ndata_gt_train = load_MIMIC(\"/kaggle/input/evaluation-data/MIMIC_train.json\", \"MIMIC_gt_train\", save_csv=True)\ndata_gt_dev = load_MIMIC(\"/kaggle/input/evaluation-data/MIMIC_dev.json\", \"MIMIC_gt_dev\", save_csv=True)\ndata_gt_test = load_MIMIC(\"/kaggle/input/evaluation-data/MIMIC_test.json\", \"MIMIC_gt_test\", save_csv=True, test=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Queries from Knowledge Graphs**","metadata":{}},{"cell_type":"code","source":"# Process data for the CARe-BERT pipeline\ndata_pipeline.progress_apply(lambda x: create_queries(x, qdr_dict, vocabulary), axis='columns')\n\n# Update vocabulary with combined categories\nfor key in vocabulary:\n    vocabulary[key]['ALL'] = list(vocabulary[key]['located_at'] | vocabulary[key]['described_as'] | vocabulary[key]['suggestive_of'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Negatives and Form Triplets**","metadata":{}},{"cell_type":"code","source":"# Create negatives and triplets\nfor query, embed_docs in tqdm(qdr_dict.items()):\n    create_triplets(query, embed_docs)\n\n# Prepare training triplets\nfor qid, query in tqdm(queries.items()):\n    train_queries[qid] = {\n        'qid': qid,\n        'query': query,\n        'pos': pos[qid],\n        'neg': neg[qid]\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuning S-BERT","metadata":{}},{"cell_type":"code","source":"# Dataloader class\nclass Dataset(Dataset):\n    def __init__(self, queries, corpus):\n        self.queries = queries\n        self.queries_ids = list(queries.keys())\n        self.corpus = corpus\n\n        for qid in self.queries:\n            self.queries[qid]['pos'] = list(self.queries[qid]['pos'])\n            self.queries[qid]['neg'] = list(self.queries[qid]['neg'])\n            random.shuffle(self.queries[qid]['neg'])\n\n    def __getitem__(self, item):\n        query = self.queries[self.queries_ids[item]]\n        query_text = query['query']\n\n        pos_id = query['pos'].pop(0)\n        pos_text = self.corpus[pos_id]\n        query['pos'].append(pos_id)\n\n        neg_id = query['neg'].pop(0)\n        neg_text = self.corpus[neg_id]\n        query['neg'].append(neg_id)\n        \n        return InputExample(texts=[query_text, pos_text, neg_text])\n\n    def __len__(self):\n        return len(self.queries)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into train, validation, and test sets\nall_keys = list(train_queries.keys())\nrandom.shuffle(all_keys)\n\ntrain_keys = all_keys[:int(len(train_queries) * 0.4)]\nval_keys = all_keys[int(len(train_queries) * 0.4):int(len(train_queries) * 0.5)]\ntest_keys = all_keys[int(len(train_queries) * 0.5):int(len(train_queries) * 0.6)]\n\nqueries_subset_train = {k: train_queries[k] for k in train_keys}\nqueries_subset_val = {k: train_queries[k] for k in val_keys}\nqueries_subset_test = {k: train_queries[k] for k in test_keys}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transfer Learning**","metadata":{}},{"cell_type":"code","source":"# Configure logging\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    level=logging.INFO,\n    handlers=[logging.StreamHandler()]\n)\n\n# Parse arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--train_batch_size\", default=64, type=int)\nparser.add_argument(\"--max_seq_length\", default=100, type=int)\nparser.add_argument(\"--model_name\", required=True)\nparser.add_argument(\"--max_passages\", default=0, type=int)\nparser.add_argument(\"--epochs\", default=10, type=int)\nparser.add_argument(\"--pooling\", default=\"mean\")\nparser.add_argument(\"--negs_to_use\", default=None)\nparser.add_argument(\"--optimizer_class\", default=\"AdamW\")\nparser.add_argument(\"--lr\", default=2e-5, type=float)\nparser.add_argument(\"--warmup_steps\", default=100, type=int)\nparser.add_argument(\"--weight_decay\", default=0.01, type=float)\nparser.add_argument(\"--use_pre_trained_model\", default=True, action=\"store_true\")\nparser.add_argument(\"--use_all_queries\", default=False, action=\"store_true\")\nargs = parser.parse_args([\"--model_name\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"])\n\nprint(args)\n\nmodel_name = args.model_name\nmodel_save_path = f'{model_name.replace(\"/\", \"-\")}-{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n\n# Load or create SBERT model\nif args.use_pre_trained_model:\n    print(\"Using pretrained SBERT model\")\n    model = SentenceTransformer(model_name)\n    model.max_seq_length = args.max_seq_length\nelse:\n    print(\"Creating new SBERT model\")\n    word_embedding_model = models.Transformer(model_name, max_seq_length=args.max_seq_length)\n    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), args.pooling)\n    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n# Prepare datasets and evaluators\ntrain_dataset = Dataset(queries_subset_train, corpus=corpus)\ndev_dataset = Dataset(queries_subset_val, corpus=corpus)\ntest_dataset = Dataset(queries_subset_test, corpus=corpus)\n\ndev_evaluator = TripletEvaluator.from_input_examples(dev_dataset, name='dev')\ntest_evaluator = TripletEvaluator.from_input_examples(test_dataset, name='test')\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.train_batch_size)\ntrain_loss = losses.TripletLoss(model=model)\n\n# Evaluate before fine-tuning\nprint(\"Performance before fine-tuning:\")\nprint(dev_evaluator(model))\n\n# Train the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    evaluator=dev_evaluator,\n    epochs=args.epochs,\n    warmup_steps=args.warmup_steps,\n    weight_decay=args.weight_decay,\n    use_amp=True,\n    checkpoint_path=model_save_path,\n    checkpoint_save_steps=len(train_dataloader),\n    optimizer_params={'lr': args.lr}\n)\n\n# Save the model\nmodel.save(model_save_path)\n\n# Evaluate the model on the test set\nprint(\"Evaluating model on test set\")\nprint(model.evaluate(test_evaluator))","metadata":{},"execution_count":null,"outputs":[]}]}